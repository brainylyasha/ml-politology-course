{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRGcihvn80cG",
        "colab_type": "text"
      },
      "source": [
        "## Методы машинного обучения\n",
        "### Домашнее задание 3. Случайный лес и градиентный бустинг\n",
        "\n",
        "В этом задании вам предстоит воспроизвести подход случайного леса через одно решающее дерево и поэкспериментировать со случайным лесом, а также настроить параметры бустинга.\n",
        "\n",
        "Задание выполняется самостоятельно, плагиат будет стандартно наказываться лишением всех баллов за задание.\n",
        "- Максимальная оценка за задание: 10 баллов.\n",
        "- Дата выдачи: 08.03.2020\n",
        "- Жесткий дедлайн: 23:59 18.03.2020\n",
        "- Мягкого дедлайна на этот раз нет."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj6ONiRZ6-uj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi0Z3q3W9irD",
        "colab_type": "text"
      },
      "source": [
        "## Часть 1. Случайные леса"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NARi_72E9npF",
        "colab_type": "text"
      },
      "source": [
        "Случайный лес — алгоритм машинного обучения, представляющий собой бэггинг над решающими деревьями (усреднение ответов множества слабых алгоритмов) с двумя основными идеями:\n",
        "- Использование подмножества признаков при построении каждого разбиения в дереве.\n",
        "- Бутстрап обучающей выборки для построения каждого дерева (случайный выбор объектов с повторениями).\n",
        "\n",
        "В этом задании мы попробуем оценить пользу каждой из идей. Будем использовать [ту же выборку](https://www.kaggle.com/iabhishekofficial/mobile-price-classification) с тем же разбиением на две части, что и в домашнем задании про логистическую регрессию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa2Jh19B-ROu",
        "colab_type": "text"
      },
      "source": [
        "### Подготовка данных\n",
        "\n",
        "Решается задача многоклассовой классификации — определение ценовой категории телефона. Для простоты перейдём к задаче бинарной классификации — пусть исходные классы 0 и 1 соответствуют классу 0 новой целевой переменной, а остальные — классу 1.\n",
        "\n",
        "Повторите следующие стадии подготовки данных (можно скопировать из задания про логистическую регрессию):\n",
        "\n",
        "- замените целевую переменную, отделите её в отдельную переменную и удалите из исходной выборки\n",
        "- разделите выборку на обучающую и тестовую части в соотношении 7 к 3. Для этого можно использовать `train_test_split` [из scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Не забудьте зафиксировать `random_state` для разбиения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N40_tVIN9c3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln90lOiM_LB-",
        "colab_type": "text"
      },
      "source": [
        "Для начала обучите решающее дерево `DecisionTreeClassifier` [из scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) на обучающей выборке и посчитайте [ROC-AUC](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) и [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) (порог 0.5) на тестовой. Не забудьте зафиксировать `random_state` для построения дерева (несмотря на то, что в классической реализации никакой случайности нет, при большой глубине дерева может возникать неоднозначность в выборке признака в разбиении). Используйте этот `random_state` для всех заданий ниже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu1qrRzM_Kk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK83TQRjAKWw",
        "colab_type": "text"
      },
      "source": [
        "### 1. Только усреднение классификаторов (2 балла)\n",
        "\n",
        "Реализуйте бэггинг над решающими деревьями (усреднение предсказанных вероятностей всего ансамбля). \n",
        "В качестве базового алгоритма используйте всё тот же `DecisionTreeClassifier`. Количество базовых алгоритмов предлагается брать равным 100. Отдельный класс делать необязательно — можно просто сложить правильно обученные деревья (одна и та же выборка, разные random_state) в список и усреднить предсказания.\n",
        "\n",
        "Посчитайте качество с помощью тех же метрик. Ответьте на следующие вопросы:\n",
        "- Что интересного вы видите?\n",
        "- С чем это связано?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyEJFeM6_h7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXFBb6qJDOju",
        "colab_type": "text"
      },
      "source": [
        "### 2. Сэмплирование обучающей выборки (2 балла)\n",
        "\n",
        "Добавим к нашему усреднению предсказаний бутстрап выборки (генерация случайной выборки того же размера с возвращением). Для этого может пригодиться `numpy.random.randint`. Сгенерируйте с помощью него отдельную обучающую выборку для каждого дерева, обучите их и усредните предсказания, как в предыдущем пункте.\n",
        "\n",
        "Посчитайте качество. Как оно изменилось по сравнению с усреднением обычных деревьев из предыдущего пункта?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_TaseSfC-W7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(123)  # для одинакового бутстрапа в каждом запуске\n",
        "\n",
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5-tYSYyEIgw",
        "colab_type": "text"
      },
      "source": [
        "### 3. Выбор случайного подмножества признаков (2 балла)\n",
        "\n",
        "Временно забудем о бутстрапе выборки и добавим выбор случайного подмножества признаков при построении каждого разбиения. В `DecisionTreeClassifier` за это отвечает параметр `max_features`. По умолчанию он имеет значение `None`, что обозначает использование всех возможных признаков. Для задачи классификации рекоменуется использовать квадратный корень от количества признаков. Попробуйте выставить такое значение. На этот раз надо отключить фиксированный `random_state` в построении дерева, так как иначе каждый раз мы будем выбирать одинаковые подмножества признаков. Обучите каждое дерево на одной и той же выборке, но с разными подмножествами признаков.\n",
        "\n",
        "Посчитайте качество. Что вы видите?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SHb3yvDEChG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(123)  # для воспроизводимости построения случайных подмножеств признаков\n",
        "\n",
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnLjOYmrFCeN",
        "colab_type": "text"
      },
      "source": [
        "### 4 = 2 + 3 (1 балл)\n",
        "\n",
        "Объединим два подхода (бутстрап + выбор подмножества признаков). Получим случайный лес. Обучите каждое дерево на индивидуальной выборке, полученной с помощью бутстрепа, и используйте случайные подмножества признаков.\n",
        "\n",
        "Посчитайте качество. Что вы видите?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpsNugj4E9-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBBMxK8kFJ_N",
        "colab_type": "text"
      },
      "source": [
        "То, что мы сделали, уже реализовано в `RandomForestClassifier`. Попробуйте воспользоваться им. Количество используемых деревьев передаётся в параметре `n_estimators`.\n",
        "\n",
        "Посчитайте качество. Что вы видите?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30OgPA3tFGVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0QwqPDoFNZV",
        "colab_type": "text"
      },
      "source": [
        "### 5. Влияние количества деревьев в случайном лесе (2 балла)\n",
        "\n",
        "Один из параметров случайного леса — количество деревьев, используемых в бэггинге. Оценим, как влияет этот параметр на финальное качество. Для этого обучите случайные леса с разным количество деревьев (например, перебирайте от 10 до 1000 с шагом в 10), оцените качество с помощью ROC-AUC. Постройте график зависимости ROC-AUC от количества используемых деревьев. Что вы видите?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DybSBzaFFM8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDnVbzgdFWbM",
        "colab_type": "text"
      },
      "source": [
        "### 6. Важность признаков (1 балл)\n",
        "\n",
        "Случайный лес позволяет оценить важность признаков. У обученного случайного леса есть аттрибут `feature_importances_`, где хранится важность для каждого признака. Постройте `barplot` с важностью признаков (удобно использовать библиотеку `seaborn`, где можно для каждого столбца передать название признака `train.columns`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8xPlVQ2FUFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYJ-t8Je-H7X",
        "colab_type": "text"
      },
      "source": [
        "## Выводы\n",
        "\n",
        "Напишите, что интересного вы узнали в этой работе, в каких экспериментах какие результаты получились.\n",
        "\n",
        "- ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iji3oAmO-TK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}