{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "practice-homework-02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GlrvEOB-1Mu",
        "colab_type": "text"
      },
      "source": [
        "# Методы машинного обучения\n",
        "\n",
        "## Домашнее задание 2. Логистическая регрессия и градиентный спуск.\n",
        "\n",
        "В этом задании вам предстоит поработать с логистической регрессией. Полная ее реализация, конечно же, не потребуется, однако за нее можно заработать бонусные баллы.\n",
        "\n",
        "На все вопросы требуется отвечать развёрнуто, аппелируя к полученным значениям или графикам, ответы вписывать в отдельную ячейку, выбрав для неё тип «Markdown». От полноты и качества ответов будет во многом зависеть ваша итоговая оценка.\n",
        "\n",
        "- Максимальная оценка за задание: 10 баллов.\n",
        "- Дата выдачи: 8.02.2020\n",
        "- Мягкий дедлайн: 23:59 MSK 22.02.2020\n",
        "- Жесткий дедлайн: 23:59 MSK 26.02.2020\n",
        "\n",
        "    Для сдачи задания необходимо загрузить в Anytask заполненную .ipynb-тетрадку до мягкого дедлайна.\n",
        "    Также можно сдать задание после мягкого, но до жесткого дедлайна, при этом за каждые прошедшие сутки после мягкого дедлайна из итоговой оценки за задание вычитывается 1 балл.\n",
        "    Работы, присланные после жесткого дедлайна, не оцениваются.\n",
        "    Если вы нашли решение какой-то задачи в открытом источнике, вам нужно не только переписать оттуда решение, но и указать ссылку на этот источник. Возможно, такое решение нашли сразу несколько студентов, и это обезопасит вас от подозрений в плагиате.\n",
        "    При обнаружении плагиата обнуляются работы всех задействованных в списывании студентов и подается докладная записка в деканат.\n",
        "    Вопросы по домашнему заданию стоит задавать в канале в Телеграме. Очевидно, готовыми решениями там (и где бы то ни было) делиться нельзя.\n",
        "\n",
        "Задание состоит из задач, каждая из них имеет определенную «стоимость» (указана в скобках около задачи). В конце работы есть бонусное задание.\n",
        "\n",
        "Удачи!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDpwg9QV-1M0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pylab inline\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj6q-bz4-1NC",
        "colab_type": "text"
      },
      "source": [
        "## Часть 0. Подготовка данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSFIBk7W-1NF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Для наших экспериментов возьмём обучающую выборку (файл `train.csv`) [отсюда](https://www.kaggle.com/iabhishekofficial/mobile-price-classification)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjHGT4oO-1NI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFNyvbOQ-1NQ",
        "colab_type": "text"
      },
      "source": [
        "Решается задача многоклассовой классификации — определение ценовой категории телефона. Для простоты перейдём к задаче бинарной классификации — пусть исходные классы 0 и 1 соответствуют классу 0 новой целевой переменной, а остальные классу 1.\n",
        "\n",
        "Замените целевую переменную, сохраните её в отдельную переменную и удалите из исходной выборки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHKzXAG8-1NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INWvEWaI-1Na",
        "colab_type": "text"
      },
      "source": [
        "Разделите выборку на обучающую и тестовую части в соотношении 7 к 3. Для этого можно использовать [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) из scikit-learn. Не забудьте зафиксировать сид для разбиения (выставить параметр `random_state`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsP0XnnH-1Nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojYvmlx7-1Nj",
        "colab_type": "text"
      },
      "source": [
        "## Часть 1. Логистическая регрессия.\n",
        "\n",
        "[10 баллов]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPbhyahN-1Nl",
        "colab_type": "text"
      },
      "source": [
        "В этой части вы будете обучать самый простой бинарный классификатор — логистическую регрессию. Будем использовать готовую реализацию [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) из scikit-learn.\n",
        "\n",
        "Логистическая регрессия — линейный метод, то есть в нём предсказание алгоритма вычисляется как скалярное произведение признаков и весов алгоритма: \n",
        "\n",
        "$$\n",
        "b(x) = w_0 + \\langle w, x \\rangle = w_0 + \\sum_{i=1}^{d} w_i x_i\n",
        "$$\n",
        "\n",
        "Для вычисления вероятности положительного класса применяется сигмода. В результате предсказание вероятности принадлежности объекта к положительному классу можно записать как: \n",
        "\n",
        "$$\n",
        "P(y = +1 | x) = \\frac{1}{1 + \\exp(- w_0 - \\langle w, x \\rangle )}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TayJJRKK-1Nn",
        "colab_type": "text"
      },
      "source": [
        "**1. [1 балл]** Не забывайте, что для линейных методов матрицу объекты-признаки необходимо предварительно отмасштабировать (то есть привести каждый признак к одному и тому же масштабу одним из способов). Отмасштабируйте обучающую и тестовую выборки, полученные в предыдущих пунктах (например, при помощи [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) — для этого необходимо сначала вызвать метод `fit_transform` для обучающей выборки, а затем метод `transform` для тестовой — их нужно вызывать именно таким образом, чтобы информация из тестовой выборки не протекала в обучение). \n",
        "\n",
        "Напоминаем — метод `.fit` у большинства классов в sklearn или обучает алгоритм, или находит нужные параметры для преобразования (в нашем случае среднее и стандартное отклонение по признакам), а `.transform` — применяет преобразование к выборке. Объединенный метод `.fit_transform` делает оба шага сразу."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uwJSvU7-1Np",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaXa6mp8-1Nv",
        "colab_type": "text"
      },
      "source": [
        "**2. [3 балла]** Обучите логистическую регрессию при помощи `LogisticRegression` на обучающей выборке. Сделайте предсказания для тестовой выборки, посчитайте по ним [ROC-AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) и [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) (для порога, равного 0.5). Хорошо ли удаётся предсказывать целевую переменную для тестовой выборки?\n",
        "\n",
        "Не забывайте, что метод `predict_proba` вычисляет вероятности обоих классов выборки, а в бинарной классификации нас интересует в первую очередь вероятность принадлежности к положительному классу."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T_l482B-1Nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lDIAfuM-1N3",
        "colab_type": "text"
      },
      "source": [
        "**3. [2 балла]** У обученной логистической регрессии есть два атрибута: `coef_` и `intercept_`, которые соответствуют весам $w$ и $w_0$. Это и есть результат обучения логистической регрессии. Попробуйте с помощью них (с помощью всё той же обученной ранее логистической регрессии) посчитать «сырое» предсказание алгоритма $b(x) = w_0 + \\langle w, x \\rangle = w_0 + \\sum_{i=1}^{d} w_i x_i$. Вам может помочь функция `np.dot()`.\n",
        "\n",
        "Постройте гистограмму полученных значений и ответьте на вопросы:\n",
        "- Какие значения принимает такое предсказание?\n",
        "- Похожи ли эти значения на вероятность классов?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFGPKbN2-1N4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRSSqFIc-1N-",
        "colab_type": "text"
      },
      "source": [
        "**4. [1 балл]** Реализуйте функцию сигмоиды $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ и постройте её график. Что вы можете сказать об этой функции и чем она может быть нам полезна?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ltdFbUo-1N_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riD6OWGA-1OF",
        "colab_type": "text"
      },
      "source": [
        "**5. [1 балл]** Примените реализованную сигмоиду к $b(x)$ из п.3. Вы должны получить вероятности принадлежности к положительному классу. Проверьте, что ваши значения совпали с теми, которые получены с помощью `predict_proba`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjaFBmVK-1OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZUSulVQ-1ON",
        "colab_type": "text"
      },
      "source": [
        "Таким образом, обучение логистической регрессии — настройка параметров $w$ и $w_0$, а применение — подсчёт вероятностей принадлежности положительному классу как применение сигмоды к скалярному произведению признаков и параметров.\n",
        "\n",
        "**6. [2 балл]** Постройте для обученной логистической регрессии ROC-кривую [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) и PR-кривую [precision_recall_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STYF0ZtM-1OO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3llcNVcY-1OT",
        "colab_type": "text"
      },
      "source": [
        "### Бонусное задание. Обучение логистической регрессии.\n",
        "\n",
        "[до 5 бонусных баллов]\n",
        "\n",
        "Если выше вручную мы только применяли логистическую регрессию, то здесь предлагается реализовать обучение с помощью полного градиентного спуска.\n",
        "\n",
        "Если кратко, то обучение логистической регрессии с $L_2$-регуляризацией можно записать в виде задачи оптимизации:\n",
        "\n",
        "$$\n",
        "Q(w, X) = \\frac{1}{l} \\sum_{i=1}^{l} \\log (1 + \\exp(- y_i \\langle w, x_i \\rangle )) + \\lambda_2 \\lVert w \\rVert _2^2 \\to \\min_w\n",
        "$$\n",
        "\n",
        "Считаем, что $y_i \\in \\{-1, +1\\}$, а нулевым признаком сделан единичный (то есть $w_0$ соответствует свободному члену). Искать $w$ будем с помощью градиентного спуска, тогда вектор весов на итерации $t+1$ будет вычисляться по следующей формуле: \n",
        "\n",
        "$$\n",
        "w^{(t+1)} = w^{(t)} - \\alpha \\nabla_w Q(w, X)\n",
        "$$\n",
        "\n",
        "Длину шага $\\alpha > 0$ в рамках данного задания предлагается брать равной некоторой малой константе (например, $\\alpha=10^{-6}$).\n",
        "\n",
        "Градиент функционала из задачи выше считается по следующей формуле:\n",
        "\n",
        "$$\n",
        "\\nabla_w Q(w, X) = - \\frac{1}{l} \\sum_{i=1}^{l} \\frac{y_i x_i}{1 + \\exp(y_i \\langle w, x_i \\rangle)} + 2 \\lambda_2 w\n",
        "$$\n",
        "\n",
        "На самом деле неправильно регуляризировать свободный член $w_0$ (то есть при добавлении градиента для $w_0$ не надо учитывать слагаемое с $\\lambda_2$). Но в рамках этого задания мы не обращаем на это внимания и работаем с $w_0$ так же, как с остальными элементами вектора $w$.\n",
        "\n",
        "В качестве критерия останова необходимо использовать (одновременно):\n",
        "- проверку на евклидовую норму разности весов на двух соседних итерациях (например, меньше некоторого малого числа порядка $10^{-6}$, которое лежит в параметре `tolerance`), т.е. условие будет иметь вид $\\| w^{(t+1)} - w^{(t)} \\| < 10^{-6}$ (или похожий)\n",
        "- достижение максимального числа итераций (например, 10000 — число, которое лежит в параметре `max_iter`)\n",
        "\n",
        "Инициализировать веса (т.е. вектор $w^{0}$) можно случайным образом или нулевым вектором.\n",
        "\n",
        "Реализуйте обучение логистической регрессии. Для удобства ниже предоставлен прототип с необходимыми методами. В `loss_history` необходимо сохранять вычисленное на каждой итерации значение функции потерь. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqPSewA7-1OV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class LogReg(BaseEstimator):\n",
        "    def __init__(self, lambda_2=1.0, tolerance=1e-4, max_iter=1000, alpha=1e-3):\n",
        "        \"\"\"\n",
        "        lambda_2: L2 regularization param\n",
        "        tolerance: for stopping gradient descent\n",
        "        max_iter: maximum number of steps in gradient descent\n",
        "        alpha: learning rate\n",
        "        \"\"\"\n",
        "        self.lambda_2 = lambda_2\n",
        "        self.tolerance = tolerance\n",
        "        self.max_iter = max_iter\n",
        "        self.alpha = alpha\n",
        "        self.w = None\n",
        "        self.loss_history = None\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: self\n",
        "        \"\"\"\n",
        "        self.loss_history = []\n",
        "        \n",
        "        pass\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d)\n",
        "        ---\n",
        "        output: np.array of shape (l, 2) where\n",
        "        first column has probabilities of -1\n",
        "        second column has probabilities of +1\n",
        "        \"\"\"\n",
        "        if self.w is None:\n",
        "            raise Exception('Not trained yet')\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    def calc_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: np.array of shape (d)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def calc_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: float \n",
        "        \"\"\" \n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU-wkS_z-1OZ",
        "colab_type": "text"
      },
      "source": [
        "1. Обучите и примените логистическую регресию на тех же выборках.\n",
        "2. Посчитайте качество по тем же метрикам.\n",
        "3. Визуализируйте изменение значений функции потерь от номера итераций. Получившийся график должен строго убывать. Объясните, почему ожидается такой результат."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PABS_cgx-1Oa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9oTqviL-1PL",
        "colab_type": "text"
      },
      "source": [
        "## Выводы\n",
        "\n",
        "Напишите, что интересного вы узнали в этой работе, в каких экспериментах какие результаты получились.\n",
        "\n",
        "- ..."
      ]
    }
  ]
}